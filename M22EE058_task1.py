# -*- coding: utf-8 -*-
"""Task1_Assignment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-V7UNIuUs-Q4qlCnyuqPFa78nItzDLeP

**a.) K-means clustering from scratch**

**Analysing dataset**
"""

import numpy as np
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

train_data = pd.read_csv('/content/drive/MyDrive/mnist_train.csv')
test_data = pd.read_csv('/content/drive/MyDrive/mnist_test.csv')

train_data.shape

test_data.shape

train_data.describe()

"""**Removing labels from dataset**"""

X_train = train_data.iloc[:, 1:].values
y_train = train_data.iloc[:, 0].values
X_test = test_data.iloc[:, 1:].values
y_test = test_data.iloc[:, 0].values

"""**Checking dimension after label removal**"""

X_train.shape

"""**Normalising dataset**"""

X_train = X_train/255.0

X_train_df = pd.DataFrame(X_train)
X_train_df.describe()

"""**Plotting labels corresponding to first nine samples**"""

import matplotlib.pyplot as plt
plt.gray()
plt.figure(figsize = (10,9))

for i in range(9):
 plt.subplot(3,3,i+1)
 image = X_train[i].reshape(28, 28)
 plt.imshow(image)

"""**K-means clustering code from scratch using cosine similarity as distance metrics for 10, 7 and 4 clusters**"""

def cal_cosine_similarity(X, centroids):
    dot_product = np.dot(X, centroids.T)
    norm_X = np.linalg.norm(X, axis=1)
    norm_centroids = np.linalg.norm(centroids, axis=1)
    cosine_similarity = dot_product / (norm_X[:, np.newaxis] * norm_centroids)
    return cosine_similarity

def kmeans_clustering(X, n_clusters, max_iters=100, random_state=None):
    if random_state is not None:
        np.random.seed(random_state)

    centroids = X[np.random.choice(range(len(X)), size=n_clusters, replace=False)]

    for _ in range(max_iters):
        cosine_similarity = cal_cosine_similarity(X, centroids)
        cluster_labels = np.argmax(cosine_similarity, axis=1)
        new_centroids = np.array([np.mean(X[cluster_labels == i], axis=0) for i in range(n_clusters)])

        if np.all(centroids == new_centroids):
            break

        centroids = new_centroids

    return cluster_labels, centroids

num_clusters = [10, 7, 4]
cluster_assignments = []
for n_clusters in num_clusters:
    cluster_labels, _ = kmeans_clustering(X_train, n_clusters, random_state=0)
    cluster_assignments.append(cluster_labels)

"""**b.) Visualization of image clustered in 10, 7 and 4 clusters**"""

def visualize_clustered_images(cluster_assignments, num_clusters, X_train):
    for i, n_clusters in enumerate(num_clusters):
        fig, axes = plt.subplots(n_clusters, 5, figsize=(10, 8))
        for j in range(n_clusters):
            cluster_indices = np.where(cluster_assignments[i] == j)[0]
            random_indices = np.random.choice(cluster_indices, size=5, replace=True)
            plt.suptitle(f'K(Clusters)={n_clusters}')
            for k, idx in enumerate(random_indices):
                image = X_train[idx].reshape(28, 28)
                axes[j, k].imshow(image, cmap='gray')
                axes[j, k].axis('off')
                if k==0:
                  axes[j, k].set_title(f'Cluster {j}')
                axes[j, k].axis('off')
        plt.tight_layout()
        plt.show()
visualize_clustered_images(cluster_assignments, num_clusters, X_train)

"""**d.) Finding optimal number of clusters for this dataset**

**Elbow Method**

From the plotted graph, we can see that the elbow point is for K=4. So, optimal number of cluster for the given dataset can be 4.
"""

def within_cluster_sum_of_squares(X, centroids, cluster_labels):
  wcss=0
  for i in range(len(centroids)):
    cluster_points = X[cluster_labels==i]
    centroid = centroids[i]
    wcss += np.sum(np.linalg.norm(cluster_points - centroid, axis=1)**2)
    return wcss

def cal_optimal_clusters(X, max_clusters):
  wcss_values = []
  for k in range (1, max_clusters+1):
    cluster_labels, centroids = kmeans_clustering(X, k, random_state=0)
    wcss = within_cluster_sum_of_squares(X, centroids, cluster_labels)
    wcss_values.append(wcss)

  plt.figure(figsize=(10, 6))
  plt.plot(range(1, max_clusters + 1), wcss_values, marker='o', linestyle='-', color='b')
  plt.title('Elbow Method')
  plt.xlabel('Number of Clusters (K)')
  plt.ylabel('Within-Cluster Sum of Squares (WCSS)')
  plt.grid(True)
  plt.show()


max_clusters = 15
cal_optimal_clusters(X_train, max_clusters)

"""**Silhouette Score Method**"""

from sklearn.metrics import silhouette_score

silhouette_scores = []
for n_clusters in range(2, 11):
    cluster_labels,_ = kmeans_clustering(X_train, n_clusters, random_state=0)
    silhouette_avg = silhouette_score(X_train, cluster_labels)
    silhouette_scores.append(silhouette_avg)

optimal_num_clusters = silhouette_scores.index(max(silhouette_scores)) + 2

print("Optimal Number of Clusters:", optimal_num_clusters)